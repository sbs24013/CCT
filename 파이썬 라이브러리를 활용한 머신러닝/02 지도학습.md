# 02 지도학습

## 2.1 분류와 회귀

지도학습

- 지도학습은 분류와 회귀로 나눌수 있다

- 분류(classification)
  - 미리 정의된, 가능성이 있는 클래스 레이블 중 하나를 예측하는 것
  - 이진 분류(binary classification): 두 개의 클래스로 분류
    - ex. 스팸 메일 분류
  - 자둥 분류(multiclass classification): 셋 이상의 클래스로 분류
    - ex. 붓꽃 분류기
  - 출력 값에 연속성이 없다
- 회귀(regression)
  - 연속적인 숫자, 부동소수점(실수)를 예측하는 것
  - ex. 교육 수준, 나이, 주거지를 바탕으로 연간 소득을 예측하는 것
  - 출력 값에 연속성이 있다

## 2.2 일반화, 과대적합, 과소적합

일반화(generalization)

- 모델이 **처음 보는 데이터에 대해 정확하게 예측**할 수 있으면, 훈련 세트에서 테스트 세트로 일반화가 되었다고 한다
- 훈련 세트에 맞는 복잡한 모델을 만드는 경우, 훈련 세트에"만" 정확한 모델이 될 수 있다

과대적합과 과소적합

- 과대적합(overfitting): 가진 정보를 모두 사용해서 너무 복잡한 모델을 만드는 것
- 과소적합(underfitting): 데이터의 다양성을 반영하지 않고 너무 간단한 모델을 만드는 것

우리가 찾는 모델은 "일반화 성능"이 최대가 되는 최적점에 있는 모델이다

![모델 복잡도에 따른 훈련과 테스트 전확도의 변화](2-1.png)

### 2.2.1 모델 복잡도와 데이터셋 크기의 관계

모델의 복잡도는 훈련 데이터셋에 담긴 입력 데이터의 다양성과 관련이 깊다

다양한 데이터 포인터가 많을수록 과대적합 없이 더 복잡한 모델을 만들 수 있다.

## 2.3 지도 학습 알고리즘

### 2.3.1 예제에 사용할 데이터셋

데이터셋을 사용하기 위한 준비

```py
# mglearn 패키지를 설치
%pip install mglearn

# import packages
import mglearn
import matplotlib.pyplot as plt

```

forge 데이터셋

- 인위적으로 만든 이진 분류 데이터셋
- 특성 2개와 26개의 포인터를 갖는다

```py

X, y = mglearn.datasets.make_forge()
mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
plt.legend(["클래스 0", "클래스 1"], loc=4)
plt.xlabel("첫 번째 특성")
plt.ylabel("두 번째 특성")

print("X.shape:", X.shape)
# X.shape: (26, 2)
```

![forge 데이터셋의 산점도](2-2.png)

wave 데이터셋

- 회귀 알고리즘
- 입력 특성 하나와 모델링할 타깃 변수(또는 응답)을 가진다
- 특성을 x축에 놓고 회귀의 타깃(출력)을 y축에 놓는다

```py
# wave 데이터셋
# 회귀 알고리즘에 사용

X, y = mglearn.datasets.make_wave(n_samples=40)
plt.plot(X, y, 'o')
plt.ylim(-3, 3)
plt.xlabel("Features")
plt.ylabel("Target")
```

![x 축을 특성, y축을 타깃으로 한 wave 데이터셋의 그래프](image.png)

scikit-learn의 cancer 데이터셋

- 각 종양은 양성과 악성으로 레이블되어 있고
- 조직 데이터를 기반으로 종양이 악성인지 예측할수 있도록 학습하는 것이 과제

```py
from sklearn.datasets import load_breast_cancer
cancer = load_breast_cancer()
print("cancer.keys():\n", cancer.keys())

# cancer.keys():
# dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])
```

scikit-learn에 포함된 데이터셋은 Bunch 객체에 저장되어 있다.

- 딕셔너리와 비슷하지만 점표기법 사용가능
- bunch['key'] 대신 bunch.key 사용 가능

```py
# 569개의 데이터 포인트
# 30개의 특성

print("유방암 데이터의 형태", cancer.data.shape)

# Output
# 유방암 데이터의 형태 (569, 30)
```

```py
# 569 개 데이터 포인터 중
# 212 개는 악성이고
# 357 개는 양성이다

print("클래스별 샘플 개수:\n", {
  n: v for n, v in zip(cancer.target_names, np.bincount(cancer.target))
})

# Output
# 클래스별 샘플 개수:
#  {'malignant': 212, 'benign': 357}
```

```py
# feature_names 속성: 각 특성의 의미를 알 수 있다
print("특성 이름:\n", cancer.feature_names)

# 특성 이름:
#  ['mean radius' 'mean texture' 'mean perimeter' 'mean area'
#  'mean smoothness' 'mean compactness' 'mean concavity'
#  'mean concave points' 'mean symmetry' 'mean fractal dimension'
#  'radius error' 'texture error' 'perimeter error' 'area error'
#  'smoothness error' 'compactness error' 'concavity error'
#  'concave points error' 'symmetry error' 'fractal dimension error'
#  'worst radius' 'worst texture' 'worst perimeter' 'worst area'
#  'worst smoothness' 'worst compactness' 'worst concavity'
#  'worst concave points' 'worst symmetry' 'worst fractal dimension']
```

보스턴 주택 가격

- 회귀 분석용 실제 데이터셋
- 범죄율, 찰스강 인접도, 고속도로 접근성 등의 정보를 이용해 1970 년대 보스턴 주변의 주택 평균 가격을 예측
- 506 포인트와 13 특성이 있다

```py
# 보스턴 주택 가격 데이터셋
data_url = "http://lib.stat.cmu.edu/datasets/boston"
raw_df = pd.read_csv(data_url, sep="\s+", skiprows=22, header=None)
data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
target = raw_df.values[1::2, 2]
print("데이터의 형태:", data.shape)

# Output
# 데이터의 형태: (506, 13)
```

확장된 데이터셋

- 13개의 입력특성 외에도 특성끼리 곱하여 (= 상호작용) 의도적으로 확장 하였다 = 특성 공학(feature engineering)
- 13 + 91 = 104

```py
X, y = mglearn.datasets.load_extended_boston()
print("X.shape:", X.shape)

# Output
# X.shape: (506, 104)
```

### 2.3.2 k-최근접 이웃

- K-NN; k-Nearest Neighbors
- 가장 간단한 머신러닝 알고리즘
- 훈련 데이터트를 그냥 저장하는 것이 모델을 만드는 과정의 전부이다
- 새로운 데이터 포인트에 예측을 할 때 알고리즘이 훈련 데이터셋에서 가장 가까운 포인트, 즉 '최근접 이웃'을 찾는다

**k-최근접 이웃 분류**

- 가장 가까운 훈련 데이터 포인트를 하나를 최근접 이웃으로 찾아 예측에 사용

```py
# forge 데이터셋
mglearn.plots.plot_knn_classification(n_neighbors=1)
```

![forge 데이터셋에 대한 1-최근접 이웃 모델의 예측](2-4.png)

- 데이터 포인트 3개를 추가 (별)
- 추가한 데이터에 가장 가까운 훈련 데이터 포인터를 연결
- 1-KNN 알고리즘의 예측은 이 데이터의 레이블이 된다

하나가 아닌 임의의 k개를 선택할 수 있다

- 둘 이상의 이웃을 선택할 때는 레이블을 정하기 위한 투표를 한다
- 클래스 별로 속한 이웃이 몇 개인지 세알리고, 이웃이 많은 클래스를 레이블로 지정한다

```py
mglearn.plots.plot_knn_classification(n_neighbors=3)
```

![forge 데이터셋에 대한 3-KNN 모델의 예측](2-5.png)

클래스가 많은 경우에도 이웃에 포함된 클래스의 갯수를 세알려 가장 많은 클래스를 예측값으로 사용한다

**scikit-learn을 사용하여 k-NN 알고리즘 적용**

```py
# 일반화 성능을 평가할 수 있도록 데이터를 훈련 세트와 테스트 세트로 나눔
from sklearn.model_selection import train_test_split
X, y = mglearn.datasets.make_forge()

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

# KNeighborsClassifier 클래스를 임포트하고 객체를 만든다
# 이웃의 수 같은 매개변수를 지정한다
from sklearn.neighbors import KNeighborsClassifier
clf = KNeighborsClassifier(n_neighbors=3)

# 훈련 세트를 사용하여 분류 모델을 학습시킨다
# KNeighborsClassifier 학습: 예측할 때 이웃을 찾을 수 있도록 데이터를 저장
clf.fit(X_train, y_train)

# 테스트 데이터에 대해 predict 메서드를 호출하여 예측한다
# 각 데이터 포인트에 대해 훈련 세트에서 가장 가까운 이웃을 계산한 다음 가장 많은 데이터를 찾는다
print("테스트 세트 예측:", clf.predict(X_test))
# Output
# 테스트 세트 예측: [1 0 1 0 1 0 0]

# 일반화 평가
# score 메서드에 테스트 데이터와 테스트 레이블을 넣어 호출한다
print("테스트 세트 정확도: {:.2f}".format(clf.score(X_test, y_test)))
# Output
# 테스트 세트 정확도: 0.86
# 86% 정확도; 모델이 테스트 데이터셋에 있는 샘플 중 86%를 정확히 예측하였다
```

**KNeighborsClassifier 분석**

- 2차원 데이터셋이므로 가능한 모든 테스트 포인트의 예측을 xy 평면에 그려볼 수 있다
- 각 데이터 포인트에 속한 클래스에 따라 평면을 칠하면 지정한 영역으로 나뉘는 결정 경계 (decision boundary)를 볼 수 있다.

```py
# Decision boundary
fig, axes = plt.subplots(1, 3, figsize=(10, 3))

for n_neighbors, ax in zip([1, 3, 9], axes):
  # fit: sef 오브젝트를 리턴
  # 객체 생성과 fot 메서드를 한줄에 쓸 수 있다
  clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X, y)
  # 결정 경계
  mglearn.plots.plot_2d_separator(clf, X, fill=True, eps=0.5, ax=ax, alpha=.4)
  # 분산도
  mglearn.discrete_scatter(X[:, 0],X[:,1], y,ax=ax)
  # 타이틀 & 레전드
  ax.set_title("{} Neighbor".format(n_neighbors))
  ax.set_xlabel("Feature 0")
  ax.set_ylabel("Feature 1")

axes[0].legend(loc=3)
```

![n_neighbors 값이 각기 다른 최근접 이웃 모델이 만든 결정 경계](./images/2-6.png)

- 이웃 1: 훈련 데이터에 가깝다 -> 복잡도가 올라가고
- 이웃 9: 경계가 부드러워 진다 -> 복잡도가 낮아진다

**유방암 데이터셋을 사용하여 모델의 복잡도와 일반화 사이의 관계를 입증**

```py
# 1. 테스트 세트와 훈련 세트를 나누고
# 2. 이웃수를 달리하여 훈련 세트와 테스트 세트의 성능을 평가
```

2024 03 12 5pm
p. 67
